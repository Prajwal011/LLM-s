LLM Course

This repository contains materials for an introductory course on Large Language Models (LLMs).

Course Overview:

This course will provide a comprehensive introduction to LLMs, covering their history, fundamental NLP concepts, and key architectures like Transformers. We will explore various methods for accessing and interacting with LLMs, including prompt engineering, fine-tuning, and Retrieval Augmented Generation (RAG). Additionally, we will delve into the concept of LLM agents and discuss methods for evaluating the performance of trained models.

Course Structure:

Chapter 1:Introduction to Large Language Models (LLMs)



-The history of LLMs



Chapter 2: Fundamentals of Natural Language Processing (NLP)



-Padding

-Truncation

- Creating tensors of tokens



Chapter 3: Getting Started with Transformers



- How do Transformers work?

-Transformers

-Types of transformers



Chapter 4: Accessing Models



-text generation

- text to text generation

-question answering

-text to image

-text classification

-Conclusion

- Further Ways to Access some LLMS

- Customizing Your LLM: A Guide to Prompt Engineering, RAG, Fine-tuning, and Pretraining



Chapter 5: Prompt engineering

- Techniques to prompt engineering open source models

Chapter 6: Fine tuning llm models



-Quantization in LLM

-bitsandbytes

-Load Model and Tokenizer

-Traditional Fine-Tuning vs. LoRA

-QLoRA parameters

-TrainingArguments

-Supervised Fine-Tuning with SFT (Text-Perturbed Linear Encoder-Function)

-Futher Finetuning notebooks



Chapter 7: RAG(Retrieval Augmented Generation)



-Retrieval Augmented Generation (RAG)

-Further Reading



Chapter 8: LLM Agents

- introduction to llm agent

- building llm agents

Chapter 9:Evaluation of trained model.

-llm evaluating metrics

-llm evaluation using langsmith



We welcome contributions from the community! Please feel free to submit pull requests for bug fixes, improvements, and new content.
