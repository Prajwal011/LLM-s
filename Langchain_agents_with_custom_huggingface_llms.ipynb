{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwal011/LLM-s/blob/main/Langchain_agents_with_custom_huggingface_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5ARqsLzLM_W",
        "outputId": "c3c097b9-95aa-480f-9745-df11e895894b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.14-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.30 (from langchain)\n",
            "  Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.37 (from langchain)\n",
            "  Downloading langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.40-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.37->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.14 langchain-community-0.0.31 langchain-core-0.1.40 langchain-text-splitters-0.0.1 langsmith-0.1.40 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.0 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.29.1-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: You might need to restart the session after the pip install block"
      ],
      "metadata": {
        "id": "WNBjn-QC-y5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
        "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast"
      ],
      "metadata": {
        "id": "iiefB_6WJqYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-5mUr765MGiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "c0c593c03a964270b665b5290c42a60a",
            "7d4a41c7aff442c091170a7a4fd3721d",
            "fbe05f93dd7a4b6aa49461692739e532",
            "6dc1d08dc7f74bd59d3c4e77375c723c",
            "e1724ac6b74842beb699d1e31387411a",
            "00ab964e29c24259b9ae6bddb768c8e4",
            "9d82c6578dfa4e0c85d9d9305819128f",
            "2d71f2e009f843fb93ec067c74bcdb87",
            "4c4c0ad7e00945f6a26bcdbc0cfe2581",
            "1ebb8055cdf344b19f51adee9ed01f6f",
            "3b0707c98f854b86acbd93a9a148783b",
            "89cea55de3504cf7ae7a1e8fe930d673",
            "3043c17bcd5a4e55bbebe2816e25362b",
            "b368e5890343426da3919e88a3256b12",
            "f26d725b385140ae8072889811cddbcd",
            "3966ec3d84944e61955abdef9d6820e8",
            "addb7a0362f34adf86e50d2fa6cc91a0",
            "89a354426bcd43069ef2a56f7327e56a",
            "f0b4a085e56a4686bc3f80f8b2298450",
            "1d777cc0a6274a029d7c26e39da2c5c1",
            "c029588ae4544d6b956d0e655220999d",
            "e2522fc182a34c988930c8560cb529b5",
            "70fc89f5dd4144969670897c8b01d1d5",
            "30f788d5023a4f689351769d8a219589",
            "94a31bd1bba248269f703e24775821c9",
            "55367c0c7c1f4512ac91604f12d98690",
            "558c62d8a69c48d4903e1b4f125f30b5",
            "bfbb8e459dbf4a34b66506a7178a9b20",
            "050da96dd3f54880830b6834bedefa2c",
            "214bb06e05914deead7c551f20e105d1",
            "41855b940a5240b594e802a78c97708d",
            "ae9d5773829147878b2f258e6b0f79ae",
            "32c27701e7594f1daca7727e45d08714",
            "ce8672e29d58439e977ad782dc5333f2",
            "ef42c8327c56443888deed9ab86f0140",
            "700f1b111b634c33bd651d3a07e4e536",
            "f8a4204d074c45d0be046f20a27b0b02",
            "b31cf5c90ce64ca68487a636a08f6e0f",
            "d0f01c1becd04a5eb7705b7707d2486d",
            "c174d2b2990e4bc98dff602a01aedac1",
            "c16c894c3e8e4366b3de74b5ff4bae63",
            "e65cb0bbaa534c96a5ece677d384d581",
            "29b2703622534a2eb5d187dac9a0df64",
            "3614543c66b04299bb5396a798bde520",
            "3c79248334874a33b057ac4d45c28487",
            "79c7d9c3d5de4afc9235abd70c0282bd",
            "e7a62ffa0a994529bdbfbe04107d97bd",
            "839e7ffb6d184854a4ef390f63c0623a",
            "7cec2fcf521c406384ac6dd3d761545e",
            "6ce89c7b2d7347519e0a3c3e425307f0",
            "ac84907197e840adbafdde5e285d6797",
            "abada74e7d7948b4af8278cc0f1fc4e7",
            "7919dc82c268477c995799f6eb7c8812",
            "01a48b2fabc047289055ece61c2bfc25",
            "5e247cc4bb0043efabf84c04e8d6bc71",
            "7c73db98415d449daecc9737c5ece6bd",
            "d89246246fb14a3592b81977709054f3",
            "eb8a4693e73346888afc78a4e4334d48",
            "e241ac7a9a654f259bff4fad4ef4d864",
            "30e3e29da081419399eafc5ba36d866e",
            "9afd5e6246fa4840a40946e51c68fe92",
            "36f3b88bf91c41bcaeeac907742a2231",
            "d7154de6309c47f3bb862576095990f1",
            "dfce68787500438c99ddaf9b9d22c8dd",
            "d6e32b0bb9d04262aac6aafa1f1c2d9b",
            "87ec3c72ae264d13bf511dde37c789eb",
            "252e3794be324e4cb9012d9c7436a354",
            "8b473539efeb4761a4099387c9ca621c",
            "f6e994f7c650479093637515b91944ad",
            "ef375417bc9b489baaf2de842eb1e202",
            "7d0f49848a814151b9a37ade0a4b6230",
            "825bf8b8cb7d40b9a1358b0ec6498da0",
            "bac0d0b407164f03809b906e2ced4f46",
            "a91b52da1d0e46acaddf39e56cfde272",
            "e7a30d8214b643b095cd96b4891a626f",
            "50d89ecf829c48a7a440edbd8c9da646",
            "b240ee301d3e4d1bbf25c9b1f08de0a3",
            "b50de3e5cdf347e5aab60f094a68bb8c",
            "72400b38fdf444dd828883137a582c92",
            "23f43f5413574003a1c9da6c64f66094",
            "82a4b522c9374f3f99c37f2a7e51e806",
            "d0d281d0de3a4ccbbcf338fa643fec7c",
            "7d347d14055047bc8e3b65c778ac13ee",
            "48857749e5384cfca0dde016aa629d0a",
            "9ff0e9893aef468cbbe64be6fe11d6ab",
            "cbfa0381090644d8814d28d2eddde4b7",
            "cdfead5806ec44ff9a5cb3f69a8e0a81",
            "57c02ceef2ac4a1ba9c9f6ae95335e5d",
            "581c3e7160f640829916dd31e6a7f8c7",
            "a87ba0ef836b4f09b173d776f4403f36",
            "eee3332b37194897a89d720281a1d723",
            "cd80870815c6404082529225d66464b3",
            "bb5446bdb5d5405a99e882fe039ca507",
            "9abc0b659b45401f91a4ac7eaec55d0c",
            "a4e4665c267d41adb19fd02660ebd2a8",
            "c0b7ca02f5414b2c836fcc017f18232c",
            "fdd5b782e63d45a1bae6206c809d6f64",
            "dbd418a47d0e4ea3910c35c294e861f4",
            "ee73e11a610048bb85278d10574f0fdd",
            "6cbcdf9df15444b8a7eb14cc68c4d92f",
            "c9fcec87f9da4af0bfd493580f52de4b",
            "cb25ed10b9fa47be8a5c72a3949512f3",
            "c7ab54127fb9408c9cd00cf69e157197",
            "2ce0b7f13c2d450997b9116c1f6b32a3",
            "d32f1cc602234d77b480f65a865c78ba",
            "fd707a87d9f941b3881b8367bd2fbc8d",
            "214bdd2a8637420c888455de15c4feec",
            "d299fbe545184c46a18bb729ee1f8cfd",
            "51f9fdd0c9964533bd010a47eaa00668",
            "d797eb12ae4c41ffb9bcd3c65f916901",
            "41c72237f77f43228a54f554dad18932",
            "7e2e776a81d44309b67b84f5b83e8ef0",
            "c1e964e22e7848019438cbe7cd5d4f17",
            "2fb9dd87d34648e19c2b7fc40ea74b9d",
            "861082a1d5d047eb9f8dda13d98ac7f1",
            "93d7ee1016e942af97aa28b588cab39b",
            "05f8a128cefc4d39b63cc048816e624a",
            "6f8c6472cf1c4833aee4f8c71193a550",
            "ad54e26275154750b22775a874c0ee30",
            "9002872bca43436cb041057c88b419c2",
            "e636de4725bf4491ae6b03f6315fdf80",
            "178c1cc88b864ca39357c56fca000fbe",
            "285337b26b1c40159d2d63d325d17f39",
            "41b74bdb1c0049e08e913471d084eda4",
            "9cabf8b8f0884916bc52bafba59b9356",
            "6cc24940d70a4d34b8b4ad13242923ab",
            "374147daabd84ff6ac961a5a6afbc200",
            "6bd22e97eb494bafb178dfc57d5bf397",
            "ba44f27f57ff4a73934d6f335aa79357",
            "00beeb2b5d024ebfb823b273ef13ef22",
            "00db3c1cc2e14fdf994cdb0f56868608",
            "d16e0c5d6c0f46e6848a4c168c63fa61"
          ]
        },
        "id": "WUt3vhKm0-L8",
        "outputId": "4b7d245d-ca82-466f-db95-db0384e65e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0c593c03a964270b665b5290c42a60a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89cea55de3504cf7ae7a1e8fe930d673",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70fc89f5dd4144969670897c8b01d1d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce8672e29d58439e977ad782dc5333f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c79248334874a33b057ac4d45c28487",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c73db98415d449daecc9737c5ece6bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "252e3794be324e4cb9012d9c7436a354"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b50de3e5cdf347e5aab60f094a68bb8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "581c3e7160f640829916dd31e6a7f8c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cbcdf9df15444b8a7eb14cc68c4d92f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41c72237f77f43228a54f554dad18932"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "178c1cc88b864ca39357c56fca000fbe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from typing import Optional, List, Mapping, Any"
      ],
      "metadata": {
        "id": "p8gI1imENgcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLLMMistral(LLM):\n",
        "    model: MistralForCausalLM\n",
        "    tokenizer: LlamaTokenizerFast\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:\n",
        "\n",
        "        messages = [\n",
        "         {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "\n",
        "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "        model_inputs = encodeds.to(self.model.device)\n",
        "\n",
        "        generated_ids = self.model.generate(model_inputs, max_new_tokens=512, do_sample=True, pad_token_id=tokenizer.eos_token_id, top_k=4, temperature=0.7)\n",
        "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "        output = decoded[0].split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip()\n",
        "\n",
        "        if stop is not None:\n",
        "          for word in stop:\n",
        "            output = output.split(word)[0].strip()\n",
        "\n",
        "        # Mistral 7B sometimes fails to properly close the Markdown Snippets.\n",
        "        # If they are not correctly closed, Langchain will struggle to parse the output.\n",
        "        while not output.endswith(\"```\"):\n",
        "          output += \"`\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model}"
      ],
      "metadata": {
        "id": "RVpApK370u8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = CustomLLMMistral(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "JjF6EWaH1HI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlv8MGdnSgJ_",
        "outputId": "790881d1-2ddb-4ef0-e5b6-58c45a26249a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=9725e095c69b2d5158e7f3c68b2376f85e3b1320565a9aa13904432354219623\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools"
      ],
      "metadata": {
        "id": "1-fsPHA4hfBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numexpr as ne\n",
        "from langchain.tools import WikipediaQueryRun, BaseTool\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain.agents import Tool"
      ],
      "metadata": {
        "id": "GADmKfbcaKqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We limit the number of results to 1 and the maximum number of characters to 2500. This limitation is imposed because, although Mistral 7B can support prompts of up to 32000 tokens, using a free Colab account would not provide sufficient memory for overly large inputs. But feel free to experiment changing the parameters."
      ],
      "metadata": {
        "id": "bGACkKM9hNGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=2500))"
      ],
      "metadata": {
        "id": "I7ggXzXRBOpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wikipedia.run(\"Deep Learning\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10938f58-a998-4bb2-b41e-7fde19521fc9",
        "id": "IQD_kxGoMheY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: Deep learning\n",
            "Summary: Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog. ANNs are generally seen as low quality models for brain function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia_tool = Tool(\n",
        "    name=\"wikipedia\",\n",
        "    description=\"Never search for more than one concept at a single step. If you need to compare two concepts, search for each one individually. Syntax: string with a simple concept\",\n",
        "    func=wikipedia.run\n",
        ")"
      ],
      "metadata": {
        "id": "8qXMWtcPaNMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Calculator(BaseTool):\n",
        "    name = \"calculator\"\n",
        "    description = \"Use this tool for math operations. It requires numexpr syntax. Use it always you need to solve any math operation. Be sure syntax is correct.\"\n",
        "\n",
        "    def _run(self, expression: str):\n",
        "      try:\n",
        "        return ne.evaluate(expression).item()\n",
        "      except Exception:\n",
        "        return \"This is not a numexpr valid syntax. Try a different syntax.\"\n",
        "\n",
        "    def _arun(self, radius: int):\n",
        "        raise NotImplementedError(\"This tool does not support async\")"
      ],
      "metadata": {
        "id": "PwOI2ZNKTroH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_tool = Calculator()\n",
        "calculator_tool.run(\"2+3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MD_ARaGVg8i",
        "outputId": "633ea369-f341-48ec-a43c-5e5a46fa294b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [wikipedia_tool, calculator_tool]"
      ],
      "metadata": {
        "id": "yB9hV6Eialyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing the Prompt"
      ],
      "metadata": {
        "id": "ieNdbNj9hakQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "id": "DmHtlAqctPxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system=\"\"\"\n",
        "You are designed to solve tasks. Each task requires multiple steps that are represented by a markdown code snippet of a json blob.\n",
        "The json structure should contain the following keys:\n",
        "thought -> your thoughts\n",
        "action -> name of a tool\n",
        "action_input -> parameters to send to the tool\n",
        "\n",
        "These are the tools you can use: {tool_names}.\n",
        "\n",
        "These are the tools descriptions:\n",
        "\n",
        "{tools}\n",
        "\n",
        "If you have enough information to answer the query use the tool \"Final Answer\". Its parameters is the solution.\n",
        "If there is not enough information, keep trying.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8kI0wCpDMheZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human=\"\"\"\n",
        "Add the word \"STOP\" after each markdown snippet. Example:\n",
        "\n",
        "```json\n",
        "{{\"thought\": \"<your thoughts>\",\n",
        " \"action\": \"<tool name or Final Answer to give a final answer>\",\n",
        " \"action_input\": \"<tool parameters or the final output\"}}\n",
        "```\n",
        "STOP\n",
        "\n",
        "This is my query=\"{input}\". Write only the next step needed to solve it.\n",
        "Your answer should be based in the previous tools executions, even if you think you know the answer.\n",
        "Remember to add STOP after each snippet.\n",
        "\n",
        "These were the previous steps given to solve this query and the information you already gathered:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HwGo3V3JMheZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", human),\n",
        "        MessagesPlaceholder(\"agent_scratchpad\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "iRtZ21B5dkPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Agent"
      ],
      "metadata": {
        "id": "QWR8f8i4hkte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_json_chat_agent, AgentExecutor\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "m3BemxPxRKKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_json_chat_agent(\n",
        "    tools = tools,\n",
        "    llm = llm,\n",
        "    prompt = prompt,\n",
        "    stop_sequence = [\"STOP\"],\n",
        "    template_tool_response = \"{observation}\"\n",
        ")"
      ],
      "metadata": {
        "id": "27QqSFwLbEjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ],
      "metadata": {
        "id": "FtnzYtaAMheZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, memory=memory)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
      ],
      "metadata": {
        "id": "XRLf8fkFhQbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "p6DkeQtdhojI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"How much is 23 plus 17?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55592eb4-322d-4e4c-beab-3eb5af9ff634",
        "id": "HW03b22pUC5T"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"This is a mathematical problem that can be solved using the calculator tool.\",\n",
            " \"action\": \"calculator\",\n",
            " \"action_input\": \"23 + 17\"}```\u001b[0m\u001b[33;1m\u001b[1;3m40\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The calculation was correct, the result of 23 plus 17 is 40.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"40\"}```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'How much is 23 plus 17?', 'output': '40'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"What is the capital of France?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "warEhFbHfMY4",
        "outputId": "e9b1a617-a0a0-4290-affb-9ff79583431c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The capital of France is a concept that can be found on Wikipedia.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"capital of France\"}```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Paris\n",
            "Summary: Paris is the capital and most populous city of France. With an official estimated population of 2,102,650 residents as of 1 January 2023 in an area of more than 105 km2 (41 sq mi), Paris is the fourth-most populated city in the European Union and the 30th most densely populated city in the world in 2022. Since the 17th century, Paris has been one of the world's major centres of finance, diplomacy, commerce, culture, fashion, and gastronomy. For its leading role in the arts and sciences, as well as its early and extensive system of street lighting, in the 19th century, it became known as the City of Light.The City of Paris is the centre of the Île-de-France region, or Paris Region, with an official estimated population of 12,271,794 inhabitants on 1 January 2023, or about 19% of the population of France. The Paris Region had a GDP of €765 billion (US$1.064 trillion, PPP) in 2021, the highest in the European Union. According to the Economist Intelligence Unit Worldwide Cost of Living Survey, in 2022, Paris was the city with the ninth-highest cost of living in the world.Paris is a major railway, highway, and air-transport hub served by two international airports: Charles de Gaulle Airport (the third-busiest airport in Europe) and Orly Airport. Opened in 1900, the city's subway system, the Paris Métro, serves 5.23 million passengers daily; it is the second-busiest metro system in Europe after the Moscow Metro. Gare du Nord is the 24th-busiest railway station in the world and the busiest outside Japan, with 262 million passengers in 2015. Paris has one of the most sustainable transportation systems and is one of the only two cities in the world that received the Sustainable Transport Award twice.Paris is especially known for its museums and architectural landmarks: the Louvre received 8.9. million visitors in 2023, on track for keeping its position as the most-visited art museum in the world. The Musée d'Orsay, Musée Marmottan Monet and Musée de l'Or\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The capital city of France, as per the obtained information, is Paris.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"Paris\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the capital of France?', 'output': 'Paris'}"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"Who was the inventor of the Radio?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u39Bg86r_BN",
        "outputId": "f4b6bf7a-4ff6-42f8-ba0a-8623126207ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The inventor of the radio is not a simple concept. I will search for more information using the 'wikipedia' tool.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"radio inventor\"}\n",
            "```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Invention of radio\n",
            "Summary: The invention of radio communication was preceded by many decades of establishing theoretical underpinnings, discovery and experimental investigation of radio waves, and engineering and technical developments related to their transmission and detection. These developments allowed Guglielmo Marconi to turn radio waves into a wireless communication system.\n",
            "The idea that the wires needed for electrical telegraph could be eliminated, creating a wireless telegraph, had been around for a while before the establishment of radio-based communication. Inventors attempted to build systems based on electric conduction, electromagnetic induction, or on other theoretical ideas. Several inventors/experimenters came across the phenomenon of radio waves before its existence was proven; it was written off as electromagnetic induction at the time.\n",
            "The discovery of electromagnetic waves, including radio waves, by Heinrich Rudolf Hertz in the 1880s came after theoretical development on the connection between electricity and magnetism that started in the early 1800s. This work culminated in a theory of electromagnetic radiation developed by James Clerk Maxwell by 1873, which Hertz demonstrated experimentally. Hertz considered electromagnetic waves to be of little practical value. Other experimenters, such as Oliver Lodge and Jagadish Chandra Bose, explored the physical properties of electromagnetic waves, and they developed electric devices and methods to improve the transmission and detection of electromagnetic waves. But they did not apparently see the value in developing a communication system based on electromagnetic waves.\n",
            "In the mid-1890s, building on techniques physicists were using to study electromagnetic waves, Guglielmo Marconi developed the first apparatus for long-distance radio communication. On 23 December 1900, the Canadian inventor Reginald A. Fessenden became the first person to send audio (wireless telephony) by means of electromagnetic waves, successfully transmitting over a distance of about a mile (1.6 kilometers,) and six years later on Christmas Eve 1906 he became the first person to make a public wireless broadcast.By 1910, these various wireless systems had come to be called \"radio\".\u001b[0m\u001b[32;1m\u001b[1;3mBased on the information gathered from the previous steps, it appears that Guglielmo Marconi was instrumental in the development and implementation of the first long-distance radio communication system. However, it's important to confirm this information and clarify his exact role in the invention of radio. I will search for more information using the 'wikipedia' tool.\n",
            "\n",
            "```json\n",
            "{\"thought\": \"Guglielmo Marconi's role in the invention of radio needs further clarification. I will search for more information using the 'wikipedia' tool.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"Guglielmo Marconi\"}\n",
            "```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Guglielmo Marconi\n",
            "Summary: Guglielmo Giovanni Maria Marconi, 1st Marquis of Marconi  (Italian: [ɡuʎˈʎɛlmo marˈkoːni]; 25 April 1874 – 20 July 1937) was an Italian inventor and electrical engineer, known for his creation of a practical radio wave–based wireless telegraph system. This led to Marconi being credited as the inventor of radio, and he shared the 1909 Nobel Prize in Physics with Karl Ferdinand Braun \"in recognition of their contributions to the development of wireless telegraphy\".Marconi was also an entrepreneur, businessman, and founder of The Wireless Telegraph & Signal Company in the United Kingdom in 1897 (which became the Marconi Company). In 1929, Marconi was ennobled as a Marchese (marquis) by King Victor Emmanuel III of Italy, and, in 1931, he set up Vatican Radio for Pope Pius XI.\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"Guglielmo Marconi is confirmed to be the inventor of the practical radio wave–based wireless telegraph system and a Nobel laureate. I will provide this information as the next step.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"Guglielmo Marconi is the inventor of the practical radio wave–based wireless telegraph system and a Nobel laureate.\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Who was the inventor of the Radio?',\n",
              " 'output': 'Guglielmo Marconi is the inventor of the practical radio wave–based wireless telegraph system and a Nobel laureate.'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"What is the double of the population of Madrid?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veMiVzCfgESZ",
        "outputId": "31166682-c10d-494f-b6c6-7c6a79417ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The population of Madrid is the information needed to find its double. I will use the wikipedia tool to find the population of Madrid.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"Population of Madrid\"\n",
            "}```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Madrid\n",
            "Summary: Madrid ( mə-DRID, Spanish: [maˈðɾið] ) is the capital and most populous city of Spain. The city has almost 3.4 million inhabitants and a metropolitan area population of approximately 7 million. It is the second-largest city in the European Union (EU), and its monocentric metropolitan area is the second-largest in the EU. The municipality covers 604.3 km2 (233.3 sq mi) geographical area. Madrid lies on the River Manzanares in the central part of the Iberian Peninsula at about 650 meters above mean sea level. The capital city of both Spain and the surrounding autonomous community of Madrid (since 1983), it is also the political, economic, and cultural centre of the country. The climate of Madrid features hot summers and cool winters.\n",
            "The Madrid urban agglomeration has the second-largest GDP in the European Union and its influence in politics, education, entertainment, environment, media, fashion, science, culture, and the arts all contribute to its status as one of the world's major global cities. Due to its economic output, high standard of living, and market size, Madrid is considered the major financial centre and the leading economic hub of the Iberian Peninsula and of Southern Europe. The metropolitan area hosts major Spanish companies such as Telefónica, Iberia, BBVA and FCC. It concentrates the bulk of banking operations in the country and it is the Spanish-speaking city generating the largest amount of webpages. For innovation, Madrid is ranked 19th in the world and 7th in Europe from 500 cities, in the 2022–2023 annual analysts Innovation Cities Index, published by 2ThinkNow.Madrid houses the headquarters of the UN's World Tourism Organization (UNWTO), the Ibero-American General Secretariat (SEGIB), the Organization of Ibero-American States (OEI), and the Public Interest Oversight Board (PIOB). It also hosts major international regulators and promoters of the Spanish language: the Standing Committee of the Association of Spanish Language\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"I have the population of Madrid from the previous step. I will calculate the double of the population using the calculator tool.\",\n",
            " \"action\": \"calculator\",\n",
            " \"action_input\": \"2 * 3.4e6\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3m6800000.0\u001b[0m\u001b[32;1m\u001b[1;3mAI: ```json\n",
            "{\"thought\": \"The double of the population of Madrid is 6,800,000.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"6,800,000\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the double of the population of Madrid?',\n",
              " 'output': '6,800,000'}"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following example usually fails due to limits in the model's reasoning capacity"
      ],
      "metadata": {
        "id": "J1I2_LXMOUQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"Who is older, Tom Hanks or Kevin Costner?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SC-la0D7gZ0",
        "outputId": "c86d0571-1f87-444c-eaa1-f788de104709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"I need to find the birth years of Tom Hanks and Kevin Costner to determine who is older.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"Tom Hanks\"}```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Tom Hanks\n",
            "Summary: Thomas Jeffrey Hanks (born July 9, 1956) is an American actor and filmmaker. Known for both his comedic and dramatic roles, he is one of the most popular and recognizable film stars worldwide, and is regarded as an American cultural icon. Hanks' films have grossed more than $4.9 billion in North America and more than $9.96 billion worldwide, making him the fourth-highest-grossing actor in North America. He has received numerous honors including the AFI Life Achievement Award in 2002, the Kennedy Center Honor in 2014, the Presidential Medal of Freedom and the French Legion of Honor both in 2016, as well as the Golden Globe Cecil B. DeMille Award in 2020.Hanks made his breakthrough with leading roles in a series of comedy films which received positive media attention, such as Splash (1984), The Money Pit (1986), Big (1988), and A League of Their Own (1992). He won two consecutive Academy Awards for Best Actor for starring as a gay lawyer suffering from AIDS in Philadelphia (1993) and the title character in Forrest Gump (1994). Hanks collaborated with film director Steven Spielberg on five films: Saving Private Ryan (1998), Catch Me If You Can (2002), The Terminal (2004), Bridge of Spies (2015), and The Post (2017), as well as the WW II miniseries Band of Brothers (2001), The Pacific (2010), and Masters of the Air (2024). With the former he launched his career as a director, producer, and screenwriter. He has also frequently collaborated with film directors Ron Howard, Nora Ephron, and Robert Zemeckis.\n",
            "Hanks' other films include the romantic comedies Sleepless in Seattle (1993) and You've Got Mail (1998); the dramas Apollo 13 (1995), The Green Mile (1999), Cast Away (2000), Road to Perdition (2002) and Cloud Atlas (2012); and the biographical dramas Charlie Wilson's War (2007), Captain Phillips (2013), Saving Mr. Banks (2013), Sully (2016), A Beautiful Day in the Neighborhood (2019), News of the World (2020) and Elvis (2022). He has also appear\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"I have the birth year of Tom Hanks which is 1956. I need the birth year of Kevin Costner to compare and find out who is older.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"Kevin Costner\"}```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Kevin Costner\n",
            "Summary: Kevin Michael Costner (born January 18, 1955) is an American actor, producer, and director. He has received various accolades, including two Academy Awards, three Golden Globe Awards, and a Primetime Emmy Award.\n",
            "He rose to prominence starring in such films as The Untouchables (1987), Bull Durham (1988), Field of Dreams (1989), JFK (1991), Robin Hood: Prince of Thieves (1991), The Bodyguard (1992), A Perfect World (1993), and Wyatt Earp (1994). During this time, Costner directed and starred in the western epic Dances with Wolves (1990), for which he won two Academy Awards for Best Picture and Best Director. He then starred in and co-produced Waterworld (1995) and directed The Postman (1997) and Open Range (2003).Costner's other notable films include Silverado (1985) No Way Out (1987), Tin Cup (1996), Message in a Bottle (1999), For Love of the Game (1999), Thirteen Days (2000), Mr. Brooks (2007), Swing Vote (2008), The Company Men (2010), 3 Days to Kill (2014), Draft Day (2014), Black or White (2014), McFarland, USA (2015), and The Highwaymen (2019). He has also played supporting parts in such films as The Upside of Anger (2005), Man of Steel (2013), Jack Ryan: Shadow Recruit (2014), Hidden Figures (2016), Molly's Game (2017), and Let Him Go (2020).\n",
            "On television, Costner portrayed Devil Anse Hatfield in the miniseries Hatfields & McCoys (2012), winning the Primetime Emmy Award for Outstanding Lead Actor in a Limited or Anthology Series or Movie. Since 2018, he has starred as John Dutton on the Paramount Network original drama series Yellowstone for which he received a Screen Actors Guild Award nomination and a Golden Globe award.\n",
            "\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mAI: ```json\n",
            "{\"thought\": \"I have the birth year of Tom Hanks which is 1956 and the birth year of Kevin Costner which is 1955. I can now compare the two to determine who is older.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"Tom Hanks is older than Kevin Costner.\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Who is older, Tom Hanks or Kevin Costner?',\n",
              " 'output': 'Tom Hanks is older than Kevin Costner.'}"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    }
  ]
}