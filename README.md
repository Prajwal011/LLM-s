LLM Course

This repository contains materials for an introductory course on Large Language Models (LLMs).

Course Overview:

This course will provide a comprehensive introduction to LLMs, covering their history, fundamental NLP concepts, and key architectures like Transformers. We will explore various methods for accessing and interacting with LLMs, including prompt engineering, fine-tuning, and Retrieval Augmented Generation (RAG). Additionally, we will delve into the concept of LLM agents and discuss methods for evaluating the performance of trained models.

Course Structure:

Chapter 1: Introduction to Large Language Models (LLMs)
History of LLMs
Chapter 2: Fundamentals of Natural Language Processing (NLP)
Padding
Truncation
Creating tensors of tokens
Chapter 3: Getting Started with Transformers
How do Transformers work?
Transformers
Types of transformers
Chapter 4: Accessing Models
Text generation
Text-to-text generation
Question answering
Text-to-image
Text classification
Conclusion
Further Ways to Access some LLMs
Customizing Your LLM: A Guide to Prompt Engineering, RAG, Fine-tuning, and Pretraining
Chapter 5: Prompt engineering
Techniques to prompt engineering open source models
Chapter 6: Fine tuning llm models
Quantization in LLM
bitsandbytes
Load Model and Tokenizer
Traditional Fine-Tuning vs. LoRA
QLoRA parameters
TrainingArguments
Supervised Fine-Tuning with SFT (Text-Perturbed Linear Encoder-Function)
Futher Finetuning notebooks
Chapter 7: RAG(Retrieval Augmented Generation)
Retrieval Augmented Generation (RAG)
Further Reading
Chapter 8: LLM Agents
Introduction to LLM Agents
Building LLM Agents
Chapter 9: Evaluation of trained model.
LLM evaluating metrics
LLM evaluation using langsmith


We welcome contributions from the community! Please feel free to submit pull requests for bug fixes, improvements, and new content.
