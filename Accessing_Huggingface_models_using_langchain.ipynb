{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXqmkeX+16OwThdoI5oJvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwal011/LLM-s/blob/main/Accessing_Huggingface_models_using_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Required Packages\n",
        "Ensure that you have the necessary packages installed by running the following command:"
      ],
      "metadata": {
        "id": "dgzQiFC6XCeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "29pZ3q2OWoyz"
      },
      "outputs": [],
      "source": [
        "! pip install -q transformers langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Step 2: Set Up Environment\n",
        "Make sure you have a Hugging Face Access Token saved as an environment variable HUGGINGFACEHUB_API_TOKEN.\n"
      ],
      "metadata": {
        "id": "83fvW00bW8e5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Step 3: Instantiate an LLM\n",
        "Choose one of the options to instantiate the LLM based on your preference:"
      ],
      "metadata": {
        "id": "X1qDR9RoXIFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1: HuggingFaceTextGenInference\n"
      ],
      "metadata": {
        "id": "ES0RAnZhYQgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceTextGenInference\n",
        "import os\n",
        "\n",
        "ENDPOINT_URL = \"<YOUR_ENDPOINT_URL_HERE>\"\n",
        "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "llm = HuggingFaceTextGenInference(\n",
        "    inference_server_url=ENDPOINT_URL,\n",
        "    max_new_tokens=512,\n",
        "    top_k=50,\n",
        "    temperature=0.1,\n",
        "    repetition_penalty=1.03,\n",
        "    server_kwargs={\n",
        "        \"headers\": {\n",
        "            \"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "P2M0oMwYXJP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 2: HuggingFaceEndpoint\n"
      ],
      "metadata": {
        "id": "S23kW3SgYKCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n",
        "ENDPOINT_URL = \"<YOUR_ENDPOINT_URL_HERE>\"\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=ENDPOINT_URL,\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 50,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "    },\n"
      ],
      "metadata": {
        "id": "ehP_azYCYKqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 3: HuggingFaceHub\n"
      ],
      "metadata": {
        "id": "MKWGXp2uYUst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 30,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "eUm1L7IpYVQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 4: HuggingFacePipeline\n",
        "\n",
        "Using from_model_id Method\n",
        "\n",
        "You can load a model by specifying the model ID and task using the from_model_id method."
      ],
      "metadata": {
        "id": "8IMgUpPNYYXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
        ")"
      ],
      "metadata": {
        "id": "WaeaWRZNYY5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 5: HuggingFacePipeline\n",
        "\n",
        "Passing an Existing Transformers Pipeline Directly\n",
        "\n",
        "Alternatively, you can create an existing transformers pipeline and pass it directly to the HuggingFacePipeline constructor."
      ],
      "metadata": {
        "id": "WLrS7fKLYf1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "Ntvzghu_YgF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run models"
      ],
      "metadata": {
        "id": "BiS7DvNwYu4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"question?\")"
      ],
      "metadata": {
        "id": "s7TpFGLeYv-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}