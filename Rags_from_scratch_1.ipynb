{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWgSD9xLs3C15yIF52k9RW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwal011/LLM-s/blob/main/Rags_from_scratch_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U sentence-transformers scikit-learn"
      ],
      "metadata": {
        "id": "H0MhDk_mPBSf"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using SentenceTransformers for Embedding"
      ],
      "metadata": {
        "id": "ffR0YhfIlBvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from warnings import filterwarnings\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "# 1. Initialize the model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# 2. List of sentences to encode (the \"database\")\n",
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Artificial intelligence is fascinating.\",\n",
        "    \"Python is a great programming language.\",\n",
        "    \"I enjoy building models with data.\",\n",
        "    \"Natural language processing is a part of AI.\"\n",
        "]\n",
        "\n",
        "# 3. Convert sentences into embeddings (vectors)\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "# 4. Function to find similar sentences using different similarity metrics\n",
        "def find_similar_sentences(query, metric='cosine', top_n=3):\n",
        "    # Encode the query sentence\n",
        "    query_embedding = model.encode([query])\n",
        "\n",
        "    # Select similarity metric\n",
        "    if metric == 'cosine':\n",
        "        similarities = cosine_similarity(query_embedding, sentence_embeddings)\n",
        "    elif metric == 'dot_product':\n",
        "        # Compute dot product similarity\n",
        "        similarities = np.dot(query_embedding, sentence_embeddings.T)\n",
        "    elif metric == 'euclidean':\n",
        "        # Compute Euclidean distance and convert to similarity\n",
        "        similarities = [1 / (1 + np.sum((sentence_embeddings - query_embedding)**2,axis=1))]\n",
        "    elif metric == 'manhattan':\n",
        "        # Compute Euclidean distance and convert to similarity\n",
        "        similarities = [1 / (1 + np.sum(np.abs(sentence_embeddings - query_embedding),axis=1))]\n",
        "    else:\n",
        "        print(\"Unsupported similarity metric. Choose from 'cosine', 'dot_product', 'euclidean'\")\n",
        "\n",
        "    # Get the top_n most similar sentences\n",
        "    similar_indices = np.argsort(similarities[0])[::-1][:top_n]\n",
        "    similar_sentences = [(sentences[i], similarities[0][i]) for i in similar_indices]\n",
        "\n",
        "    return similar_sentences\n",
        "\n",
        "# Example query\n",
        "query_sentence = \"I enjoy building models\"\n",
        "similar_sentences_cosine = find_similar_sentences(query_sentence, metric='cosine', top_n=2)\n",
        "similar_sentences_dot = find_similar_sentences(query_sentence, metric='dot_product', top_n=2)\n",
        "similar_sentences_euclidean = find_similar_sentences(query_sentence, metric='euclidean', top_n=2)\n",
        "similar_sentences_manhattan = find_similar_sentences(query_sentence, metric='manhattan', top_n=2)\n",
        "\n",
        "# Output the results\n",
        "print(\"Cosine Similarity:\")\n",
        "for sentence, score in similar_sentences_cosine:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score:.4f}\")\n",
        "\n",
        "print(\"\\nDot Product Similarity:\")\n",
        "for sentence, score in similar_sentences_dot:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score:.4f}\")\n",
        "\n",
        "print(\"\\nEuclidean Distance (converted to similarity):\")\n",
        "for sentence, score in similar_sentences_euclidean:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score:.4f}\")\n",
        "\n",
        "print(\"\\nManhattan Distance (converted to similarity):\")\n",
        "for sentence, score in similar_sentences_manhattan:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYxRTYLOO7K4",
        "outputId": "901a1e7f-e63b-40be-b101-bb5561ecbedd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity:\n",
            "Sentence: I enjoy building models with data., Similarity Score: 0.7920\n",
            "Sentence: I love machine learning., Similarity Score: 0.3700\n",
            "\n",
            "Dot Product Similarity:\n",
            "Sentence: I enjoy building models with data., Similarity Score: 38.3291\n",
            "Sentence: I love machine learning., Similarity Score: 20.3586\n",
            "\n",
            "Euclidean Distance (converted to similarity):\n",
            "Sentence: I enjoy building models with data., Similarity Score: 0.0449\n",
            "Sentence: I love machine learning., Similarity Score: 0.0142\n",
            "\n",
            "Manhattan Distance (converted to similarity):\n",
            "Sentence: I enjoy building models with data., Similarity Score: 0.0136\n",
            "Sentence: I love machine learning., Similarity Score: 0.0077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating embedding from scratch"
      ],
      "metadata": {
        "id": "pXVWDDsYlG4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Artificial intelligence is fascinating.\",\n",
        "    \"Python is a great programming language.\",\n",
        "    \"I enjoy building models with data.\",\n",
        "    \"Natural language processing is a part of AI.\"\n",
        "]\n",
        "\n",
        "# Step 1: Preprocess sentences\n",
        "# Remove periods and convert all sentences into lowercase\n",
        "processed_sentences = [sentence.lower().replace('.', '') for sentence in sentences]\n",
        "\n",
        "# Step 2: Create a list of all unique words in the corpus\n",
        "word_list = ' '.join(processed_sentences).split()\n",
        "\n",
        "unique_words = sorted(set(word_list))\n",
        "\n",
        "# Len of sentence with max length we'll need it for padding\n",
        "max_words = max(map(len,[i.split() for i in sentences]))\n",
        "\n",
        "# Step 3: Create encoding and decoding dictionaries for the words\n",
        "encode = {w: i for i, w in enumerate(word_list)}\n",
        "decode = {i: w for i, w in enumerate(word_list)}\n",
        "\n",
        "# Step 4: Convert sentences into embeddings (vectors)\n",
        "def sentence_to_embedding(sentence):\n",
        "    # As all our sentences are not of same length we'll convert them in same length using padding\n",
        "    embedding = [0] * max_words\n",
        "\n",
        "    # Split the sentence into words\n",
        "    words_in_sentence = sentence.split()\n",
        "\n",
        "    # Set corresponding positions to 1 for each word in the sentence\n",
        "    for i,word in enumerate(words_in_sentence):\n",
        "        if word in encode:\n",
        "            index = encode[word]\n",
        "            # print(i,word,index)\n",
        "            embedding[i] = index\n",
        "\n",
        "    return embedding\n",
        "\n",
        "# Step 5: Convert each sentence into an embedding\n",
        "sentence_embeddings = [sentence_to_embedding(sentence) for sentence in processed_sentences]\n",
        "\n",
        "# Output the results\n",
        "print(\"Unique Words (Vocabulary):\", unique_words)\n",
        "print(\"\\nSentence Embeddings:\")\n",
        "for i, embedding in enumerate(sentence_embeddings):\n",
        "    print(f\"Sentence {i+1}: {sentences[i]}\")\n",
        "    print(f\"Embedding: {embedding}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t0UKeAFqwY_",
        "outputId": "f8918e4d-d67e-47e1-e914-a84883f7ef1d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Words (Vocabulary): ['a', 'ai', 'artificial', 'building', 'data', 'enjoy', 'fascinating', 'great', 'i', 'intelligence', 'is', 'language', 'learning', 'love', 'machine', 'models', 'natural', 'of', 'part', 'processing', 'programming', 'python', 'with']\n",
            "\n",
            "Sentence Embeddings:\n",
            "Sentence 1: I love machine learning.\n",
            "Embedding: [14, 1, 2, 3, 0, 0, 0, 0]\n",
            "\n",
            "Sentence 2: Artificial intelligence is fascinating.\n",
            "Embedding: [4, 5, 23, 7, 0, 0, 0, 0]\n",
            "\n",
            "Sentence 3: Python is a great programming language.\n",
            "Embedding: [8, 23, 24, 11, 12, 21, 0, 0]\n",
            "\n",
            "Sentence 4: I enjoy building models with data.\n",
            "Embedding: [14, 15, 16, 17, 18, 19, 0, 0]\n",
            "\n",
            "Sentence 5: Natural language processing is a part of AI.\n",
            "Embedding: [20, 21, 22, 23, 24, 25, 26, 27]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using Custom Embedding with Custom Vector search index"
      ],
      "metadata": {
        "id": "8bec6gScqxoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Artificial intelligence is fascinating.\",\n",
        "    \"Python is a great programming language.\",\n",
        "    \"I enjoy building models with data.\",\n",
        "    \"Natural language processing is a part of AI.\"\n",
        "]\n",
        "\n",
        "# Step 1: Preprocess sentences\n",
        "# Remove periods and convert all sentences into lowercase\n",
        "processed_sentences = [sentence.lower().replace('.', '') for sentence in sentences]\n",
        "\n",
        "# Step 2: Create a list of all unique words in the corpus\n",
        "word_list = ' '.join(processed_sentences).split()\n",
        "\n",
        "unique_words = sorted(set(word_list))\n",
        "\n",
        "# Len of sentence with max length we'll need it for padding\n",
        "max_words = max(map(len,[i.split() for i in sentences]))\n",
        "\n",
        "# Step 3: Create encoding and decoding dictionaries for the words\n",
        "encode = {w: i for i, w in enumerate(word_list)}\n",
        "decode = {i: w for i, w in enumerate(word_list)}\n",
        "\n",
        "# Step 4: Convert sentences into embeddings (vectors)\n",
        "def sentence_to_embedding(sentence):\n",
        "    # As all our sentences are not of same length we'll convert them in same length using padding\n",
        "    embedding = [0] * max_words\n",
        "\n",
        "    # Split the sentence into words\n",
        "    words_in_sentence = sentence.split()\n",
        "\n",
        "    # Set corresponding positions to 1 for each word in the sentence\n",
        "    for i,word in enumerate(words_in_sentence):\n",
        "        if word in encode:\n",
        "            index = encode[word]\n",
        "            # print(i,word,index)\n",
        "            embedding[i] = index\n",
        "\n",
        "    return np.array(embedding)\n",
        "\n",
        "# Step 5: Convert each sentence into an embedding\n",
        "sentence_embeddings = np.array([sentence_to_embedding(sentence) for sentence in processed_sentences])\n",
        "\n",
        "# 4. Function to find similar sentences using different similarity metrics\n",
        "def find_similar_sentences(query, metric='cosine', top_n=3):\n",
        "    # Encode the query sentence\n",
        "    query_embedding = [sentence_to_embedding(query)]\n",
        "    # print(sentence_embeddings,query_embedding)\n",
        "    # Select similarity metric\n",
        "    if metric == 'cosine':\n",
        "        similarities = cosine_similarity(query_embedding, sentence_embeddings)\n",
        "    elif metric == 'dot_product':\n",
        "        # Compute dot product similarity\n",
        "        similarities = np.dot(query_embedding, sentence_embeddings.T)\n",
        "    elif metric == 'euclidean':\n",
        "        # Compute Euclidean distance and convert to similarity\n",
        "        similarities = [1 / (1 + np.sum((sentence_embeddings - query_embedding)**2,axis=1))]\n",
        "    elif metric == 'manhattan':\n",
        "        # Compute Euclidean distance and convert to similarity\n",
        "        similarities = [1 / (1 + np.sum(np.abs(sentence_embeddings - query_embedding),axis=1))]\n",
        "    else:\n",
        "        print(\"Unsupported similarity metric. Choose from 'cosine', 'dot_product', 'euclidean'\")\n",
        "\n",
        "    # Get the top_n most similar sentences\n",
        "    similar_indices = np.argsort(similarities[0])[::-1][:top_n]\n",
        "    similar_sentences = [(sentences[i], similarities[0][i]) for i in similar_indices]\n",
        "\n",
        "    return similar_sentences\n",
        "\n",
        "# Example query\n",
        "query_sentence = \"I enjoy building models\"\n",
        "similar_sentences_cosine = find_similar_sentences(query_sentence, metric='cosine', top_n=2)\n",
        "similar_sentences_dot = find_similar_sentences(query_sentence, metric='dot_product', top_n=2)\n",
        "similar_sentences_euclidean = find_similar_sentences(query_sentence, metric='euclidean', top_n=2)\n",
        "similar_sentences_manhattan = find_similar_sentences(query_sentence, metric='manhattan', top_n=2)\n",
        "\n",
        "# Output the results\n",
        "print(\"Cosine Similarity:\")\n",
        "for sentence, score in similar_sentences_cosine:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score:.4f}\")\n",
        "\n",
        "print(\"\\nDot Product Similarity:\")\n",
        "for sentence, score in similar_sentences_dot:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score:.4f}\")\n",
        "\n",
        "print(\"\\nEuclidean Distance (converted to similarity):\")\n",
        "for sentence, score in similar_sentences_euclidean:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score:.4f}\")\n",
        "\n",
        "print(\"\\nManhattan Distance (converted to similarity):\")\n",
        "for sentence, score in similar_sentences_manhattan:\n",
        "    print(f\"Sentence: {sentence}, Similarity Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-xi4Kp2nQj3",
        "outputId": "4cd65e67-f80d-480d-d2bd-0dfa1507ab27"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity:\n",
            "Sentence: Artificial intelligence is fascinating., Similarity Score: 0.8140\n",
            "Sentence: Python is a great programming language., Similarity Score: 0.7623\n",
            "\n",
            "Dot Product Similarity:\n",
            "Sentence: Natural language processing is a part of AI., Similarity Score: 1058.0000\n",
            "Sentence: Python is a great programming language., Similarity Score: 916.0000\n",
            "\n",
            "Euclidean Distance (converted to similarity):\n",
            "Sentence: Artificial intelligence is fascinating., Similarity Score: 0.0038\n",
            "Sentence: I love machine learning., Similarity Score: 0.0013\n",
            "\n",
            "Manhattan Distance (converted to similarity):\n",
            "Sentence: Artificial intelligence is fascinating., Similarity Score: 0.0312\n",
            "Sentence: I enjoy building models with data., Similarity Score: 0.0192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhQ8LKV6r0LI"
      },
      "execution_count": 73,
      "outputs": []
    }
  ]
}